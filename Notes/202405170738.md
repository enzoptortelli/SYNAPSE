---
tags:
  - "#sketch"
  - unfinished
link-tags: "[[clustering]]"
aliases:
  - Trabalho 2 clusterização
---
# Ideas

## Sobre o método
> Basing cluster analysis on a probability model has several advantages. In essence, this brings cluster analysis within the range of standard statistical methodology and makes it possible to carry out inference in a principled way

[[Bouveyron - Model based clustering and classification for data science.pdf#page=23&selection=8,0,10,64|Bouveyron - Model based clustering and classification for data science, page 23]]

> Basing cluster analysis on a probability model also leads to a way of assessing uncertainty about the clustering. In addition, it provides a systematic way of dealing with outliers by expanding the model to account for them.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=23&selection=23,0,25,65|Bouveyron - Model based clustering and classification for data science, page 23]]

> It also provides a principled way to choose the number of clusters. In fact, the choice of clustering model and of number of clusters can be reduced to a single model selection problem. It turns out that there is a trade-off between these choices. Often, if a simpler clustering model is chosen, more clusters are needed to represent the data adequately.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=23&selection=18,0,22,33|Bouveyron - Model based clustering and classification for data science, page 23]]

> Data generated by mixtures of multivariate normal densities are characterized by groups or clusters centered at the means μg , with increased density for points nearer the mean.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=37&selection=102,0,109,16|Bouveyron - Model based clustering and classification for data science, page 37]]

### Modelos de mistura de Normais multivariadas com restrições geométricas
==Falar sobre como o número de parâmetros do modelo que devemos estimar cresce rapidamente com o número de variáveis que estamos usando. Manter na cabeça a palavra PARSIMONIA==

> However, it has (G − 1) + Gd + G{d(d + 1)/2} parameters, and this can be quite a large number

[[Bouveyron - Model based clustering and classification for data science.pdf#page=37&selection=214,50,236,48|Bouveyron - Model based clustering and classification for data science, page 37]]

> Such large numbers of parameters can lead to difficulties in estimation, including lack of precision or even degeneracies. They can also lead to difficulties in interpreting the results.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=37&selection=255,52,258,12|Bouveyron - Model based clustering and classification for data science, page 37]]

> e results. In order to alleviate this problem, it is common to specify more parsimonious versions of the model. One way to do this is via the eigenvalue decomposition of the group covariance matrices Σg , in the form Σg = λg Dg Ag DT g . (2.3)

[[Bouveyron - Model based clustering and classification for data science.pdf#page=37&selection=258,2,285,5|Bouveyron - Model based clustering and classification for data science, page 37]]

==Explicar, por cima, quais as possíveis restrições geométricas que podemos aplicar no modelo.==

> geometric decomposition or Volume-Shape-Orientation (VSO) decomposition.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=39&selection=199,24,200,20|Bouveyron - Model based clustering and classification for data science, page 39]]

## Estimação por Máxima Verossimilhança
> Convergence of the EM algorithm to a local maximum of the log-likelihood function can be assessed in several ways. In essence, these consist of seeing whether the algorithm has been moving slowly in the latest iterations. One possible criterion is that the log-likelihood has changed very little between the last two iterations; a typical threshold is a change of less than 10−5. Another possible criterion is that the model parameters have changed little between the last two iterations. The R package mclust uses the log-likelihood criterion by default.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=44&selection=192,0,205,45|Bouveyron - Model based clustering and classification for data science, page 44]]

## Initializing the EM algorithm
> By default, in the mclust R package, the EM algorithm is initialized by running the hierarchical model-based clustering with the VVV model. This gives a partition for each number of clusters. These partitions are used as the initial points for the algorithm.

[[Bouveyron - Model based clustering and classification for data science.pdf#page=54&selection=55,0,68,10|Bouveyron - Model based clustering and classification for data science, page 54]]

## Steps utilized in examples (part 1)
### Example breast cancer
1. Selecionar o número de clusters e restrição geométrica;
2. Criar o modelo com Mclust();
3. Como o modelo tinha mais de 2 variáveis, para poder plotar o resultado, ele usou as duas componentes principais dos dados, utilizando a matriz de correlação, depois de escalonar os dados pelos seus desvios padrões;
4. Depois de classificados, ele fez o plot da incerteza de cada umas das observações


## Choosing the number of cluster and clustering model
> There are trade-offs between the choice of the number of clusters and that of the clustering model. If a simpler model is used, more clusters may be needed to provide a good representation of the data. If a more complex model is used, fewer clusters may suffice

[[Bouveyron - Model based clustering and classification for data science.pdf#page=66&selection=166,0,169,24|Bouveyron - Model based clustering and classification for data science, page 66]]

==O Bayesian Information Criterion (BIC) é um método para se aproximar a integral usada para escolher os parâmetros do modelo (# de clusters e restriçõeos geom).==

> he BIC is designed to choose the number of components in a mixture model rather than the number of clusters in the data set per se. The difference is subtle but important. Model-based clustering was initially grounded in the hope that the number of mixture components is the same as the number of clusters, but this may not always be true. For example, one cluster may be strongly non-Gaussian, and may itself be best represented by a mixture of normal distributions. In this case, the number of mixture components would be greater than the number of clusters

[[Bouveyron - Model based clustering and classification for data science.pdf#page=73&selection=323,1,329,77|Bouveyron - Model based clustering and classification for data science, page 73]]

> hen interest is primarily focused on clustering rather than finding the best mixture model to fit the data, one solution to this problem is to use, instead of BIC, the Integrated Completed Likelihood (ICL)

[[Bouveyron - Model based clustering and classification for data science.pdf#page=73&selection=334,1,340,5|Bouveyron - Model based clustering and classification for data science, page 73]]

## Steps utilized in examples (part 2)
### Example Old Faithful
1. Utilizou, para comparar, ambos os métodos BIC e ICL. Disse que foram considerados até 9 componentes de mistura (que não é necessariamente igual ao número de clusters (preciso ver isso melhor));
2.  Descreve a restrição geométrica e número de parâmetros livres do modelo que cada um dos método escolheu 
3. Plota as elipses de densidade do resultado e compara ambos os métodos. Ele diz que, utilizando BIC, foi obtido uma melhor estimativa da densidade (mas não entendi o porquê), enquanto que, utilizando ICL, os clusters obtidos têm uma impressão visual melhor.
4. Concluiu que é provável que tenha dois clusters, mas um deles sendo não Gaussiano, e que esse seria melhor representado por uma mistura de duas Gaussianas (que gera um cluster não Gaussiana).


# References
[[Model-based clustering and classification for Data Science]]
