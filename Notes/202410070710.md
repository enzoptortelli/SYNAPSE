---
tags:
  - "#sketch"
link-tags: 
aliases:
  - Resumo inferência
---
## Capítulo 1: Teoria de Probabilidade
### 1.2 Básico da teoria de probabilidade
**Definição**: (Sigma álgebra) Uma coleção de subconjuntos do espaço amostral $S$ é chamada de *sigma algebra* (ou *algebra de Borel*), denotada por $\mathcal{B}$ se satisfaz as seguintes propriedades:
1) $\varnothing \in \mathcal{B}$.
2) Se $A \in \mathcal{B}$, então $A^c \in \mathcal{B}$ ($\mathcal{B}$ é fechado sob complementar).
3) Se $A_1, A_2, \dots \in \mathcal{B}$, então $\cup_{i = 1}^\infty A_i \in \mathcal{B}$ ($\mathcal{B}$ é fechado sob união contável).
Repare que essa definição implica que:
1) (Pelas prop. 2 e 3 + Lei de De Morgan) Se $A_1, A_2, \dots \in \mathcal{B}$, então $\cap_{i = 1}^\infty A_i \in \mathcal{B}$ ($\mathcal{B}$ é fechado sob intersecção contável).
2) (Pelas prop. 1 e 2) $S \in \mathcal{B}$.

**Definição**: (Função de probabilidade) Dado um espaço amostral $S$ e uma sigma algebra associada $\mathcal{B}$, uma função de probabilidade é uma função $P$ com domínio em $\mathcal{B}$ que satisfaz:
1) $P(A) \geq 0$ para todo $A \in \mathcal{B}$.
2) $P(S) = 1$.
3) Se $A_1, A_2, \dots$ são conjuntos disjuntos dois a dois, então $P(\cup_{i = 1}^\infty A_1) = \sum_{i = 1}^\infty P(A_i)$.

### 1.3 Probabilidade condicional e independência
**Teorema**: Se $A$ e $B$ são eventos independentes, então também serão independentes os eventos:
1) $A$ e $B^c$,
2) $A^c$ e $B$,
3) $A^c$ e $B^c$.

**Definição**: (Independência) Uma coleção de eventos $A_1, A_2, \dots$ é mutualmente independente se, para qualquer sub coleção $A_{i_1}, A_{i_2}, \dots, A_{i_k}$, temos $$ P(\cap_{j = 1}^k A_{i_j}) = \prod_{j = i}^k P(A_{i_j}).$$
### 1.4 Variáveis aleatórias
**Definição**: Uma variável aleatória é uma função do espaço amostral $S$ para os números reais.

Uma variável aleatória define um novo espaço amostral $\mathcal{X} = \{a \in \mathbb{R} | \exists \quad s \in S \quad X(s) = a\}$, $X: S \rightarrow \mathcal{X}$ . Eu imagino que esse novo espaço amostral defina uma nova sigma-álgebra, $\mathcal{B}_X$, e uma nova função de probabilidade $P_X: \mathcal{B}_X \rightarrow [0, 1]$ (fonte: vozes da minha cabeça). 

**Teorema**: A função $P_X(X = x) = P(\{s \in S | X(s) = x\})$ é uma função de probabilidade. No caso contínuo, temos $P_X(X \in A) = P(\{s \in S | X(s) \in A\})$.

### 1.5 Função de distribuição
A **toda** variável aleatória $X$, associamos uma função chamada função distribuição acumulada de $X$.

**Definição**: (Função de probabilidade acumulada) Uma função de distribuição acumulada (fda) de uma variável aleatória $X$, denotada por $F_X(x)$, é definida por $$F_X(x) = P_X(X \leq x) \quad \forall x$$
A fato de que a fda é contínua à direita vem de sua definição. Se ela fosse definida como os valores da v.a. estritamente menores que $x$ ($<$), então ela seria contínua à esquerda.

**Teorema**: Uma função é uma fda se, e somente se,
1) $\lim_{x \rightarrow -\infty}$ 

## Capítulo 2: Transformações e Esperanças
### 2.1 Distribuições de funções de uma variável aleatória
**Definição**: Sejam $\mathcal{X} = \{x | f_X(x) > 0 \}$ e $\mathcal{Y} = \{y | f_X(x) = y \quad \text{para algum} \quad x \in \mathcal{X}\}$.

**Teorema**: Seja $X$ uma variável aleatória com função de distribuição acumulada (fda) $F_X(x)$ e seja $Y = g(X)$.  Assim,
1) Se $g$ é uma função crescente em $\mathcal{X}$, então $F_Y(y) = F_X(g^{-1}(y))$ para todo $y \in \mathcal{Y}$.
2) Se $g$ é uma função decrescente em $\mathcal{X}$ e $X$ é uma variável aleatória contínua, então $F_Y(y) = 1 - F_X(g^{-1}(y))$ para todo $y \in \mathcal{Y}$.
p.s. eu não entendi a necessidade de $X$ ser contínua para o segundo caso.

**Teorema**: 


# Reference
[[Statistical Inference]]

